# -*- coding: utf-8 -*-
"""Software Coupons Scapping Program

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w0ynbx4nzj0xAg9IkBUCPpmPLPdX-1PW
"""

import requests
from bs4 import BeautifulSoup
import string

import datetime
print(datetime.datetime.now())

!pip install pymongo

from pymongo import MongoClient
data = []
try:
    conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')
    print("Connected successfully!!!")
except:
    print("Could not connect to MongoDB")

# database
db = conn.qbytespace
collection = db.software_coupons
collection.delete_many({})
# Scrap first Five Pages
for i in range(1, 30):
  if i == 1:
    URL = 'https://winningpc.com/giveaway/'
  else:
    URL = 'https://winningpc.com/giveaway/page/'+str(i)

  headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"}
  r = requests.get(url=URL, headers=headers)
  soup = BeautifulSoup(r.content)
  datatable = soup.find('div', attrs={'class':'eq_grid pt5 rh-flex-eq-height col_wrap_three'})
# Run for loop for each page
  for row in datatable.findAll('article', attrs = {'class':'col_item offer_grid rehub-sec-smooth offer_grid_com mobile_compact_grid no_btn_enabled offer_act_enabled'}):
      quote = {}
      quote['title'] = row.h3.text
      quote['url'] = row.a['href']
      URL = row.a['href']

      headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"}
      r2 = requests.get(url=URL, headers=headers)
      soup = BeautifulSoup(r2.content)
      content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})
      paragraph = content.find_all('p')

      # Creating Another Request of URL
      # Start Coding From Here
      # End Coding from here
      quote['price'] = row.find('span', attrs={'class': 'rh_regular_price'}).text
      try:
        quote['price_before'] = row.find('del').text
      except:
        quote['price_before'] = '$100'


      a = row.find('span', attrs={'class': 'date_ago'}).text
      quote['posted_on'] = a[1:]
      quote['category'] = row.find('span',attrs={'class':'cat_link_meta'}).text
      try:
        quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text
      except:
        quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text
      quote['image'] = row.find_all('img')[0]['data-src']
      quote['description'] = [paragraph[i].text for i in range(2)]
      quote['description'] = quote['description'][0]
      bullet_points = content.find_all('ul')
      quote['lists'] = str(bullet_points[:1])
      quote['lists'] = quote['lists'].replace('\n', '')
      quote['lists'] = quote['lists'][1:-1]
      quote['created_on'] = datetime.datetime.now()
      # Appending All the data as a dictionary
      rec_id1 = collection.insert_one(quote)

      data.append(quote)

data = [
    "[<ul><li>Product name: O&amp;O AutoBackup 6.x Pro</li><li>Website: <a class=\"thirstylink\" data-nojs=\"true\" href=\"https://winningpc.com/go/oo-software/\" rel=\"nofollow noopener noreferrer sponsored\" target=\"_blank\" title=\"O&amp;O Software Homepage\">https://www.oo-software.com</a></li><li>License type: lifetime</li><li>Platform: Windows</li><li>Giveaway link: <a href=\"https://winningpc.com/go/oo-autobackup-free/\" rel=\"nofollow noopener noreferrer sponsored\" target=\"_blank\" title=\"O&amp;O AutoBackup Free\">link</a></li><li>Download: <a data-schema-attribute=\"\" href=\"https://dl5.oo-software.com/files/ooautobackup6/61/OOAutoBackup6Professional64Enu.exe\" rel=\"nofollow noopener\" target=\"_blank\">OOAutoB6Pro64Enu.exe</a>Â  <a data-schema-attribute=\"\" href=\"https://dl5.oo-software.com/files/ooautobackup6/61/OOAutoBackup6ProfessionalEnu.exe\" rel=\"noopener\" target=\"_blank\">OOAutoB6ProEnu.exe</a></li></ul>]"
]

print(data[0][1:-1])

print(data)

from pymongo import MongoClient

try:
    conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')
    print("Connected successfully!!!")
except:
    print("Could not connect to MongoDB")

# database
db = conn.qbytespace
collection = db.software_coupons



# Printing the data inserted
cursor = collection.find()
i = 0
for j in cursor:
  i+=1
print(i)
# Created or Switched to collection names: my_gfg_collection



# from pymongo import MongoClient
data = []
# try:
#     conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')
#     print("Connected successfully!!!")
# except:
#     print("Could not connect to MongoDB")

# # database
# db = conn.qbytespace
# collection = db.software_coupons
# collection.delete_many({})
# Scrap first Five Pages
for i in range(1, 2):
  if i == 1:
    URL = 'https://winningpc.com/giveaway/'
  else:
    URL = 'https://winningpc.com/giveaway/page/'+str(i)

  headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"}
  r = requests.get(url=URL, headers=headers)
  soup = BeautifulSoup(r.content)
  datatable = soup.find('div', attrs={'class':'eq_grid pt5 rh-flex-eq-height col_wrap_three'})
# Run for loop for each page
  for row in datatable.findAll('article', attrs = {'class':'col_item offer_grid rehub-sec-smooth offer_grid_com mobile_compact_grid no_btn_enabled offer_act_enabled'}):
      quote = {}
      quote['title'] = row.h3.text
      quote['url'] = row.a['href']
      URL = row.a['href']

      headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"}
      r2 = requests.get(url=URL, headers=headers)
      soup = BeautifulSoup(r2.content)
      content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})
      paragraph = content.find_all('p')

      # Creating Another Request of URL
      # Start Coding From Here
      # End Coding from here
      quote['price'] = row.find('span', attrs={'class': 'rh_regular_price'}).text
      quote['price_before'] = row.find('del').text
      a = row.find('span', attrs={'class': 'date_ago'}).text
      quote['posted_on'] = a[1:]
      quote['category'] = row.find('span',attrs={'class':'cat_link_meta'}).text

      quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text

      quote['image'] = row.find_all('img')[0]['data-src']
      quote['description'] = [paragraph[i].text for i in range(2)]
      quote['description'] = quote['description'][0]
      bullet_points = content.find_all('ul')
      quote['lists'] = str(bullet_points[:1])
      quote['lists'] = quote['lists'].replace('\n', '')
      quote['lists'] = quote['lists'][1:-1]
      # Appending All the data as a dictionary
      print(quote['brand'])
      # rec_id1 = collection.insert_one(quote)

      data.append(quote)

string ="PHVsPgo8bGk+UHJvZHVjdCBuYW1lOiBXaXNlIENhcmUgMzY1IFBSTyA2LjUuNS42Mjg8L2xpPgo8bGk+V2Vic2l0ZTogPGEgaHJlZj0iaHR0cHM6Ly93aW5uaW5ncGMuY29tL2dvL3dpc2VjbGVhbmVyLyIgcmVsPSJub29wZW5lciBzcG9uc29yZWQiIHRhcmdldD0iX2JsYW5rIj5odHRwczovL3d3dy53aXNlY2xlYW5lci5jb208L2E+PC9saT4KPGxpPkxpY2Vuc2UgdHlwZTogbGlmZXRpbWU8L2xpPgo8bGk+UGxhdGZvcm06IFdpbmRvd3M8L2xpPgo8bGk+R2l2ZWF3YXkgbGluazogbi9hPC9saT4KPGxpPkxpY2Vuc2UgY29kZSDigJMgbGFzdCB1cGRhdGVkOiBKdWx5IDIwLCAyMDIzPGJyLz48ZGl2IGNsYXNzPSJ3cHNtLWFjY29yZGlvbiBtYjMwIiBkYXRhLWFjY29yZGlvbj0ibm8iPjxkaXYgY2xhc3M9Indwc20tYWNjb3JkaW9uLWl0ZW0gY2xvc2UiPjxoMyBjbGFzcz0id3BzbS1hY2NvcmRpb24tdHJpZ2dlciI+Q2xpY2sgaGVyZS4uLjwvaDM+PGRpdiBjbGFzcz0iYWNjb3JkaW9uLWNvbnRlbnQiPgo8dWw+CjxsaT48YSBocmVmPSJodHRwczovL2Nsb3VkLmZpbGV6aWxsYS5pby93aW5uaW5ncGMvd2lzZS9XaXNlQ2FyZTM2NV9QUk9fNi41LjUtV2lubmluZ1BDLmV4ZSIgcmVsPSJub2ZvbGxvdyBub29wZW5lciIgdGFyZ2V0PSJfYmxhbmsiPldpc2VDYXJlMzY1XzYuNS41LjYyOF93aW5uaW5ncGMuZXhlPC9hPiAocHJlLXJlZ2lzdGVyZWQpPGJyLz5WaXJ1cyB0b3RhbDogPGEgaHJlZj0iaHR0cHM6Ly93d3cudmlydXN0b3RhbC5jb20vZ3VpL2ZpbGUvNzU4ODc3OTRiOWI5ZGQ0MWY5MTZhODhhNmYxMmRjMTg3Nzk0YTQwYzQzMTgzMjIyN2MwYmQ1ZGFlYTRjMGUxZCIgcmVsPSJub29wZW5lciIgdGFyZ2V0PSJfYmxhbmsiPmh0dHBzOi8vd3d3LnZpcnVzdG90YWwuY29tL2d1aS9maWxlLzc1ODg3Nzk0YjliOWRkNDFmOTE2YTg4YTZmMTJkYzE4Nzc5NGE0MGM0MzE4MzIyMjdjMGJkNWRhZWE0YzBlMWQ8L2E+PC9saT4KPC91bD4KPHA+PC9wPjwvZGl2PjwvZGl2PjwvZGl2Pgo8L2xpPgo8L3VsPg=="
string.decode()

for i in data:
  print(i['title'])

print(list(data))

len(data)

page_data = []

URL = 'https://winningpc.com/winx-dvd-ripper-platinum-coupon-code-review/'

headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36"}
r = requests.get(url=URL, headers=headers)
soup = BeautifulSoup(r.content)
content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})

paragraph = content.find_all('p')
for i in paragraph[:2]:
  print(i.text)

bullet_points = content.find_all('ul')
allpoints = []
for j in bullet_points[:1]:
  print(j)

for j in bullet_points[:1]:
  for k in j:
    print(k)

# get installation Screenshot
figure = content.find_all('figure')
figure

for i in figure:
  try:
    print(i.find('img')['data-lazy-src'])
  except:
    pass