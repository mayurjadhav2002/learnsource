{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import string"
      ],
      "metadata": {
        "id": "TxE5QJ34ApL5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4u4FSCbw_tM",
        "outputId": "30e05bed-5ce8-41f5-b328-8eaed1f29ae9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-11 20:58:22.505575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "BonZe7V9EjoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f562008-abae-40a7-e1b6-ef4dae99d47b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.4.2 pymongo-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "data = []\n",
        "try:\n",
        "    conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')\n",
        "    print(\"Connected successfully!!!\")\n",
        "except:\n",
        "    print(\"Could not connect to MongoDB\")\n",
        "\n",
        "# database\n",
        "db = conn.qbytespace\n",
        "collection = db.software_coupons\n",
        "collection.delete_many({})\n",
        "# Scrap first Five Pages\n",
        "for i in range(1, 30):\n",
        "  if i == 1:\n",
        "    URL = 'https://winningpc.com/giveaway/'\n",
        "  else:\n",
        "    URL = 'https://winningpc.com/giveaway/page/'+str(i)\n",
        "\n",
        "  headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "  r = requests.get(url=URL, headers=headers)\n",
        "  soup = BeautifulSoup(r.content)\n",
        "  datatable = soup.find('div', attrs={'class':'eq_grid pt5 rh-flex-eq-height col_wrap_three'})\n",
        "# Run for loop for each page\n",
        "  for row in datatable.findAll('article', attrs = {'class':'col_item offer_grid rehub-sec-smooth offer_grid_com mobile_compact_grid no_btn_enabled offer_act_enabled'}):\n",
        "      quote = {}\n",
        "      quote['title'] = row.h3.text\n",
        "      quote['url'] = row.a['href']\n",
        "      URL = row.a['href']\n",
        "\n",
        "      headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "      r2 = requests.get(url=URL, headers=headers)\n",
        "      soup = BeautifulSoup(r2.content)\n",
        "      content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})\n",
        "      paragraph = content.find_all('p')\n",
        "\n",
        "      # Creating Another Request of URL\n",
        "      # Start Coding From Here\n",
        "      # End Coding from here\n",
        "      quote['price'] = row.find('span', attrs={'class': 'rh_regular_price'}).text\n",
        "      try:\n",
        "        quote['price_before'] = row.find('del').text\n",
        "      except:\n",
        "        quote['price_before'] = '$100'\n",
        "\n",
        "\n",
        "      a = row.find('span', attrs={'class': 'date_ago'}).text\n",
        "      quote['posted_on'] = a[1:]\n",
        "      quote['category'] = row.find('span',attrs={'class':'cat_link_meta'}).text\n",
        "      try:\n",
        "        quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text\n",
        "      except:\n",
        "        quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text\n",
        "      quote['image'] = row.find_all('img')[0]['data-src']\n",
        "      quote['description'] = [paragraph[i].text for i in range(2)]\n",
        "      quote['description'] = quote['description'][0]\n",
        "      bullet_points = content.find_all('ul')\n",
        "      quote['lists'] = str(bullet_points[:1])\n",
        "      quote['lists'] = quote['lists'].replace('\\n', '')\n",
        "      quote['lists'] = quote['lists'][1:-1]\n",
        "      quote['created_on'] = datetime.datetime.now()\n",
        "      # Appending All the data as a dictionary\n",
        "      rec_id1 = collection.insert_one(quote)\n",
        "\n",
        "      data.append(quote)\n"
      ],
      "metadata": {
        "id": "jinWPQYMFqaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec550a7-63bb-4b4a-bb1f-49ffdb358a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected successfully!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    \"[<ul><li>Product name: O&amp;O AutoBackup 6.x Pro</li><li>Website: <a class=\\\"thirstylink\\\" data-nojs=\\\"true\\\" href=\\\"https://winningpc.com/go/oo-software/\\\" rel=\\\"nofollow noopener noreferrer sponsored\\\" target=\\\"_blank\\\" title=\\\"O&amp;O Software Homepage\\\">https://www.oo-software.com</a></li><li>License type: lifetime</li><li>Platform: Windows</li><li>Giveaway link: <a href=\\\"https://winningpc.com/go/oo-autobackup-free/\\\" rel=\\\"nofollow noopener noreferrer sponsored\\\" target=\\\"_blank\\\" title=\\\"O&amp;O AutoBackup Free\\\">link</a></li><li>Download: <a data-schema-attribute=\\\"\\\" href=\\\"https://dl5.oo-software.com/files/ooautobackup6/61/OOAutoBackup6Professional64Enu.exe\\\" rel=\\\"nofollow noopener\\\" target=\\\"_blank\\\">OOAutoB6Pro64Enu.exe</a>  <a data-schema-attribute=\\\"\\\" href=\\\"https://dl5.oo-software.com/files/ooautobackup6/61/OOAutoBackup6ProfessionalEnu.exe\\\" rel=\\\"noopener\\\" target=\\\"_blank\\\">OOAutoB6ProEnu.exe</a></li></ul>]\"\n",
        "]\n",
        "\n",
        "print(data[0][1:-1])"
      ],
      "metadata": {
        "id": "oF-1bfxHoj4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "joX_aRfznp1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "try:\n",
        "    conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')\n",
        "    print(\"Connected successfully!!!\")\n",
        "except:\n",
        "    print(\"Could not connect to MongoDB\")\n",
        "\n",
        "# database\n",
        "db = conn.qbytespace\n",
        "collection = db.software_coupons\n",
        "\n",
        "\n",
        "\n",
        "# Printing the data inserted\n",
        "cursor = collection.find()\n",
        "i = 0\n",
        "for j in cursor:\n",
        "  i+=1\n",
        "print(i)\n",
        "# Created or Switched to collection names: my_gfg_collection\n"
      ],
      "metadata": {
        "id": "GH__2HavFWQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puppfnWMnFYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pymongo import MongoClient\n",
        "data = []\n",
        "# try:\n",
        "#     conn = MongoClient('mongodb+srv://mayur:mayur--31@cluster0.v9x6kcw.mongodb.net')\n",
        "#     print(\"Connected successfully!!!\")\n",
        "# except:\n",
        "#     print(\"Could not connect to MongoDB\")\n",
        "\n",
        "# # database\n",
        "# db = conn.qbytespace\n",
        "# collection = db.software_coupons\n",
        "# collection.delete_many({})\n",
        "# Scrap first Five Pages\n",
        "for i in range(1, 2):\n",
        "  if i == 1:\n",
        "    URL = 'https://winningpc.com/giveaway/'\n",
        "  else:\n",
        "    URL = 'https://winningpc.com/giveaway/page/'+str(i)\n",
        "\n",
        "  headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "  r = requests.get(url=URL, headers=headers)\n",
        "  soup = BeautifulSoup(r.content)\n",
        "  datatable = soup.find('div', attrs={'class':'eq_grid pt5 rh-flex-eq-height col_wrap_three'})\n",
        "# Run for loop for each page\n",
        "  for row in datatable.findAll('article', attrs = {'class':'col_item offer_grid rehub-sec-smooth offer_grid_com mobile_compact_grid no_btn_enabled offer_act_enabled'}):\n",
        "      quote = {}\n",
        "      quote['title'] = row.h3.text\n",
        "      quote['url'] = row.a['href']\n",
        "      URL = row.a['href']\n",
        "\n",
        "      headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "      r2 = requests.get(url=URL, headers=headers)\n",
        "      soup = BeautifulSoup(r2.content)\n",
        "      content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})\n",
        "      paragraph = content.find_all('p')\n",
        "\n",
        "      # Creating Another Request of URL\n",
        "      # Start Coding From Here\n",
        "      # End Coding from here\n",
        "      quote['price'] = row.find('span', attrs={'class': 'rh_regular_price'}).text\n",
        "      quote['price_before'] = row.find('del').text\n",
        "      a = row.find('span', attrs={'class': 'date_ago'}).text\n",
        "      quote['posted_on'] = a[1:]\n",
        "      quote['category'] = row.find('span',attrs={'class':'cat_link_meta'}).text\n",
        "\n",
        "      quote['brand'] = row.find('span', attrs={'class':'store_post_meta_item'}).text\n",
        "\n",
        "      quote['image'] = row.find_all('img')[0]['data-src']\n",
        "      quote['description'] = [paragraph[i].text for i in range(2)]\n",
        "      quote['description'] = quote['description'][0]\n",
        "      bullet_points = content.find_all('ul')\n",
        "      quote['lists'] = str(bullet_points[:1])\n",
        "      quote['lists'] = quote['lists'].replace('\\n', '')\n",
        "      quote['lists'] = quote['lists'][1:-1]\n",
        "      # Appending All the data as a dictionary\n",
        "      print(quote['brand'])\n",
        "      # rec_id1 = collection.insert_one(quote)\n",
        "\n",
        "      data.append(quote)\n"
      ],
      "metadata": {
        "id": "P9z0QND6WaDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string =\"PHVsPgo8bGk+UHJvZHVjdCBuYW1lOiBXaXNlIENhcmUgMzY1IFBSTyA2LjUuNS42Mjg8L2xpPgo8bGk+V2Vic2l0ZTogPGEgaHJlZj0iaHR0cHM6Ly93aW5uaW5ncGMuY29tL2dvL3dpc2VjbGVhbmVyLyIgcmVsPSJub29wZW5lciBzcG9uc29yZWQiIHRhcmdldD0iX2JsYW5rIj5odHRwczovL3d3dy53aXNlY2xlYW5lci5jb208L2E+PC9saT4KPGxpPkxpY2Vuc2UgdHlwZTogbGlmZXRpbWU8L2xpPgo8bGk+UGxhdGZvcm06IFdpbmRvd3M8L2xpPgo8bGk+R2l2ZWF3YXkgbGluazogbi9hPC9saT4KPGxpPkxpY2Vuc2UgY29kZSDigJMgbGFzdCB1cGRhdGVkOiBKdWx5IDIwLCAyMDIzPGJyLz48ZGl2IGNsYXNzPSJ3cHNtLWFjY29yZGlvbiBtYjMwIiBkYXRhLWFjY29yZGlvbj0ibm8iPjxkaXYgY2xhc3M9Indwc20tYWNjb3JkaW9uLWl0ZW0gY2xvc2UiPjxoMyBjbGFzcz0id3BzbS1hY2NvcmRpb24tdHJpZ2dlciI+Q2xpY2sgaGVyZS4uLjwvaDM+PGRpdiBjbGFzcz0iYWNjb3JkaW9uLWNvbnRlbnQiPgo8dWw+CjxsaT48YSBocmVmPSJodHRwczovL2Nsb3VkLmZpbGV6aWxsYS5pby93aW5uaW5ncGMvd2lzZS9XaXNlQ2FyZTM2NV9QUk9fNi41LjUtV2lubmluZ1BDLmV4ZSIgcmVsPSJub2ZvbGxvdyBub29wZW5lciIgdGFyZ2V0PSJfYmxhbmsiPldpc2VDYXJlMzY1XzYuNS41LjYyOF93aW5uaW5ncGMuZXhlPC9hPiAocHJlLXJlZ2lzdGVyZWQpPGJyLz5WaXJ1cyB0b3RhbDogPGEgaHJlZj0iaHR0cHM6Ly93d3cudmlydXN0b3RhbC5jb20vZ3VpL2ZpbGUvNzU4ODc3OTRiOWI5ZGQ0MWY5MTZhODhhNmYxMmRjMTg3Nzk0YTQwYzQzMTgzMjIyN2MwYmQ1ZGFlYTRjMGUxZCIgcmVsPSJub29wZW5lciIgdGFyZ2V0PSJfYmxhbmsiPmh0dHBzOi8vd3d3LnZpcnVzdG90YWwuY29tL2d1aS9maWxlLzc1ODg3Nzk0YjliOWRkNDFmOTE2YTg4YTZmMTJkYzE4Nzc5NGE0MGM0MzE4MzIyMjdjMGJkNWRhZWE0YzBlMWQ8L2E+PC9saT4KPC91bD4KPHA+PC9wPjwvZGl2PjwvZGl2PjwvZGl2Pgo8L2xpPgo8L3VsPg==\"\n",
        "string.decode()\n"
      ],
      "metadata": {
        "id": "1HjL-6Rzk3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data:\n",
        "  print(i['title'])"
      ],
      "metadata": {
        "id": "vpLEWgpzVOpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(data))\n"
      ],
      "metadata": {
        "id": "hc0XChTOTpK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "xIOGneXIYLXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_data = []\n"
      ],
      "metadata": {
        "id": "Ygq-X7Xqoe7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://winningpc.com/winx-dvd-ripper-platinum-coupon-code-review/'\n",
        "\n",
        "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "r = requests.get(url=URL, headers=headers)\n",
        "soup = BeautifulSoup(r.content)\n",
        "content = soup.find('div', attrs={'class':'rh-content-wrap clearfix'})"
      ],
      "metadata": {
        "id": "hb6_orEOoh_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = content.find_all('p')\n",
        "for i in paragraph[:2]:\n",
        "  print(i.text)\n"
      ],
      "metadata": {
        "id": "6ukmeg-fpF6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bullet_points = content.find_all('ul')\n",
        "allpoints = []\n",
        "for j in bullet_points[:1]:\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "EWlDpnDEraA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in bullet_points[:1]:\n",
        "  for k in j:\n",
        "    print(k)"
      ],
      "metadata": {
        "id": "5BE8NXXEu-k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get installation Screenshot\n",
        "figure = content.find_all('figure')\n",
        "figure"
      ],
      "metadata": {
        "id": "YmoYN5TuwJXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in figure:\n",
        "  try:\n",
        "    print(i.find('img')['data-lazy-src'])\n",
        "  except:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "bDwa39nxwKGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}